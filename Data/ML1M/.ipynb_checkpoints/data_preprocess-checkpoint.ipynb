{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>itemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  itemId\n",
       "0       1    1193\n",
       "1       1     661\n",
       "2       1     914\n",
       "3       1    3408\n",
       "4       1    2355"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df = pd.read_csv('./ratings.dat', sep='::', names=[\"userId\", \"itemId\", \"rating\", \"timestamp\"])\n",
    "rating_df.drop(columns=['timestamp'], inplace=True)\n",
    "rating_df.drop(columns=['rating'], inplace=True)\n",
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996900\n",
      "996900\n"
     ]
    }
   ],
   "source": [
    "print(len(rating_df))\n",
    "rating_df.drop_duplicates(subset =['itemId', 'userId'], \n",
    "                          keep = 'first', inplace = True)\n",
    "print(len(rating_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item num = 3472\n",
      "user num = 6040\n"
     ]
    }
   ],
   "source": [
    "item_set = set(rating_df['itemId'].unique())\n",
    "user_set = set(rating_df['userId'].unique())\n",
    "print('item num = ' + str(len(item_set)))\n",
    "print('user num = ' + str(len(user_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.reset_index(drop=True, inplace=True)\n",
    "rdf_backup = copy.copy(rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = copy.copy(rdf_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iteratively remove items and users with less than 2 reviews\n",
    "rdf['user_freq'] = rdf.groupby('userId')['userId'].transform('count')\n",
    "rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')\n",
    "while np.min(rdf['user_freq']) <= 9:\n",
    "    rdf.drop(rdf.index[rdf['user_freq'] <= 9], inplace=True)\n",
    "    rdf.reset_index(drop=True, inplace=True)\n",
    "    rdf['item_freq'] = rdf.groupby('itemId')['itemId'].transform('count')\n",
    "    rdf.drop(rdf.index[rdf['item_freq'] <= 9], inplace=True)\n",
    "    rdf.reset_index(drop=True, inplace=True)\n",
    "    rdf['user_freq'] = rdf.groupby('userId')['userId'].transform('count')\n",
    "    rdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item num = 3472\n",
      "user num = 6040\n",
      "sparsity: 0.047537347025971254\n"
     ]
    }
   ],
   "source": [
    "item_list = rdf['itemId'].unique()\n",
    "user_list = rdf['userId'].unique()\n",
    "print('item num = ' + str(len(item_list)))\n",
    "print('user num = ' + str(len(user_list)))\n",
    "print('sparsity: ' + str(len(rdf) * 1.0 / (len(user_list) * len(item_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the user and item str id->int id dict\n",
    "i = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in user_list:\n",
    "    if not u in user_old2new_id_dict:\n",
    "        user_old2new_id_dict[u] = i\n",
    "        i += 1\n",
    "j = 0\n",
    "item_old2new_id_dict = dict()\n",
    "for i in item_list:\n",
    "    if not i in item_old2new_id_dict:\n",
    "        item_old2new_id_dict[i] = j\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.047537347025971254\n"
     ]
    }
   ],
   "source": [
    "print('sparsity: ' + str(len(rdf) * 1.0 / (len(user_list) * len(item_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df for rdf with int id for user and item\n",
    "for i in range(len(rdf)):\n",
    "    rdf.at[i, 'userId'] = user_old2new_id_dict[rdf.at[i, 'userId']]\n",
    "    rdf.at[i, 'itemId'] = item_old2new_id_dict[rdf.at[i, 'itemId']]\n",
    "item_list = rdf['itemId'].unique()\n",
    "user_list = rdf['userId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the df of train, vali, and test df\n",
    "rdf.reset_index(inplace=True, drop=True)\n",
    "train_df = rdf.copy()\n",
    "\n",
    "train_ratio = 0.7\n",
    "vali_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "\n",
    "vali_size = int(vali_ratio * len(rdf))\n",
    "test_size = int(test_ratio * len(rdf))\n",
    "\n",
    "vali_idx = np.random.choice(np.arange(len(train_df)), \n",
    "                            vali_size,\n",
    "                            replace=False).tolist()\n",
    "vali_df = train_df.copy()\n",
    "vali_df = vali_df.loc[vali_idx]\n",
    "train_df.drop(vali_idx, axis=0, inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_idx = np.random.choice(np.arange(len(train_df)), \n",
    "                            test_size,\n",
    "                            replace=False).tolist()\n",
    "test_df = train_df.copy()\n",
    "test_df = test_df.loc[test_idx]\n",
    "train_df.drop(test_idx, axis=0, inplace=True)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "vali_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['user_freq', 'item_freq'], inplace=True)\n",
    "test_df.drop(columns=['user_freq', 'item_freq'], inplace=True)\n",
    "vali_df.drop(columns=['user_freq', 'item_freq'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "vali_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of items users like in train, vali, test sets for each user\n",
    "\n",
    "num_item = len(item_list)\n",
    "num_user = len(user_list)\n",
    "\n",
    "user_train_like = []\n",
    "user_test_like = [] \n",
    "user_vali_like = []  \n",
    "\n",
    "train_array = train_df[['userId', 'itemId']].values\n",
    "vali_array = vali_df[['userId', 'itemId']].values\n",
    "test_array = test_df[['userId', 'itemId']].values\n",
    "\n",
    "for u in user_list:\n",
    "    train_like = (train_array[list(np.where(train_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "    vali_like = (vali_array[list(np.where(vali_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "    test_like = (test_array[list(np.where(test_array[:, 0] == u)[0]), 1]).astype(int)\n",
    "    if len(vali_like) == 0:\n",
    "        new_vali_idx = np.random.choice(np.arange(len(train_like)), size=1)\n",
    "        new_vali = train_like[new_vali_idx]\n",
    "        vali_like = np.array([new_vali])\n",
    "        train_like = np.delete(train_like, new_vali_idx)\n",
    "        train_array = np.delete(train_array, np.where((train_array[:, 0] == u) & (train_array[:, 1] == new_vali))[0], axis=0)\n",
    "        vali_array = np.append(vali_array, [[u, new_vali]], axis=0)\n",
    "    if len(test_like) == 0:\n",
    "        new_test_idx = np.random.choice(np.arange(len(train_like)), size=1)\n",
    "        new_test = train_like[new_test_idx]\n",
    "        test_like = np.array([new_test])\n",
    "        train_like = np.delete(train_like, new_test_idx)\n",
    "        train_array = np.delete(train_array, np.where((train_array[:, 0] == u) & (train_array[:, 1] == new_test))[0], axis=0)\n",
    "        test_array = np.append(test_array, [[u, new_test]], axis=0)\n",
    "        \n",
    "    user_train_like.append(train_like)\n",
    "    user_vali_like.append(vali_like)\n",
    "    user_test_like.append(test_like)\n",
    "    \n",
    "np.save('./user_train_like.npy', np.array(user_train_like))\n",
    "np.save('./user_vali_like.npy', np.array(user_vali_like))\n",
    "np.save('./user_test_like.npy', np.array(user_test_like))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'userId': train_array[:, 0], 'itemId': train_array[:, 1]})\n",
    "vali_df = pd.DataFrame({'userId': vali_array[:, 0], 'itemId': vali_array[:, 1]})\n",
    "test_df = pd.DataFrame({'userId': test_array[:, 0], 'itemId': test_array[:, 1]})\n",
    "\n",
    "\n",
    "train_df.to_csv('./train_df.csv', index=False)\n",
    "vali_df.to_csv('./vali_df.csv', index=False)\n",
    "test_df.to_csv('./test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996900"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df) + len(vali_df) + len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996900"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./info.pkl', 'wb') as f:\n",
    "    pickle.dump({'num_user': num_user, 'num_item': num_item}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
